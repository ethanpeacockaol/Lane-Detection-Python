{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f7b5042-f078-4fa6-b210-4ac40025e730",
   "metadata": {},
   "source": [
    "# Self Driving Car Eyes (.)(.)\n",
    "\n",
    "### By Ethan Peacock\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The goal of this project is to write a software pipeline to indentify the lane\n",
    "boundaries in a video with a front-facing camera on a car.  The camera<br>\n",
    "calibration images, test road images, and project videos are available<br>\n",
    "in this project repository:  https://github.com/udacity/CarND-Advanced-Lane-Lines\n",
    "\n",
    "<br>\n",
    "Below is a summary of the techniques used in this project.\n",
    "\n",
    "1. Camera calibration\n",
    "2. Distortion correction\n",
    "3. Color/gradient threshold\n",
    "4. Perspective transform\n",
    "5. Detect lane lines\n",
    "6. Determine the lane curvature\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4e150-c831-422d-9e5d-d7cee96518df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "#import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook  #  Enables interactive windows in jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff239c-529b-4719-84cb-cbc8e8676751",
   "metadata": {},
   "source": [
    "# Undistort Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481e2f0-f7c7-412f-ac45-11d3960ff5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs the camera calibration, image distortion correction and \n",
    "# returns the undistorted image        \n",
    "\n",
    "# Read in each image\n",
    "img = mpimg.imread('camera_cal/calibration1.jpg')\n",
    "# Convert image to grayscale to use for gray.shape to get image dimensions\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "def cal_undistort(img, objpoints, imgpoints):\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    \n",
    "    return undist, mtx, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860e619-0928-4b29-bf08-a7ec72fa541a",
   "metadata": {},
   "source": [
    "# Compute the camera calibration matrix and distortion coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2f5ab-b0d9-4e32-9caa-c2eeb4999041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define name of a picle file\n",
    "pickle_file = \"wide_dist_pickle.p\"\n",
    "# Read in and make a list of calibration images\n",
    "images = glob.glob('./camera_cal/*')\n",
    "\n",
    "\n",
    "# If a pickel file exists, then load the file\n",
    "if os.path.isfile(pickle_file):\n",
    "    print(\"A pickle file exists\")\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        pickle_data = pickle.load(f)\n",
    "        mtx = pickle_data['mtx']\n",
    "        dist = pickle_data['dist']\n",
    "        objpoints = pickle_data['objpoints']\n",
    "        imgpoints = pickle_data['imgpoints']\n",
    "\n",
    "        del pickle_data  # Free up memory\n",
    "# If not found, start calibrating using example images\n",
    "else:\n",
    "    # Arrays to store object points and image points from all the imgaes\n",
    "    objpoints = [] # 3D points in real world space\n",
    "    imgpoints = [] # 2D points in image plane\n",
    "\n",
    "    # Prepare object points, like (0,0,0), (1,0,0), (2,0,0), ...., (7,5,0)\n",
    "    nx = 9 # Number of inside corners of x\n",
    "    ny = 6 # Number of inside corners of y\n",
    "\n",
    "    objp = np.zeros((ny*nx,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nx,0:6].T.reshape(-1,2) # x, y coordinates\n",
    "\n",
    "    for fname in images:\n",
    "        # Read in each image\n",
    "        img = mpimg.imread(fname)\n",
    "        # Convert image to grayscale\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "\n",
    "\n",
    "        # If corners are found, add object points, image points\n",
    "        if ret == True:\n",
    "            print(fname)\n",
    "            imgpoints.append(corners)\n",
    "            objpoints.append(objp)\n",
    "\n",
    "            # Draw and display the corners\n",
    "            img = cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "            plt.imshow(img)\n",
    "            plt.savefig('output_images/draw_corners/' + str(fname))\n",
    "            plt.savefig('output_images/cornersDrawnOn.jpg')\n",
    "\n",
    "    distorted_img = mpimg.imread('camera_cal/calibration1.jpg')\n",
    "    # create mtx and dist from cal_undistort function\n",
    "    dst, mtx, dist = cal_undistort(distorted_img, objpoints, imgpoints)\n",
    "    \n",
    "    # Visualize\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(distorted_img)\n",
    "    ax1.set_title('Original Image', fontsize=50)\n",
    "    ax2.imshow(dst)\n",
    "    ax2.set_title('Undistorted Image', fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "    \n",
    "    # Save camera calibration result for later use in pickle file    \n",
    "    dist_pickle = {}\n",
    "    dist_pickle['mtx'] = mtx\n",
    "    dist_pickle['dist'] = dist\n",
    "    dist_pickle['objpoints'] = objpoints\n",
    "    dist_pickle['imgpoints'] = imgpoints\n",
    "    pickle.dump(dist_pickle, open(pickle_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8679efc-80e2-4935-80b3-83a763ac3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with test\n",
    "undist_test_image = mpimg.imread('test_images/test1.jpg')\n",
    "undist_result_test, _, _ = cal_undistort(undist_test_image, objpoints, imgpoints)\n",
    "plt.imshow(undist_test_image)\n",
    "\n",
    "# Plot the result\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "\n",
    "ax1.imshow(undist_test_image)\n",
    "ax1.set_title('Original Image', fontsize=40)\n",
    "\n",
    "ax2.imshow(undist_result_test)\n",
    "ax2.set_title('Undistorted Image', fontsize=40)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "plt.savefig('output_images/undistorted_test_img.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862d531-69d0-44e9-8629-167715adc4ea",
   "metadata": {},
   "source": [
    "# Combine Color and gradient thresholds for lane detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05acee6-7a3c-4dc0-b3bb-f96f00206118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(img, s_thresh=(170, 255), sx_thresh=(20, 100)):\n",
    "    img = np.copy(img)\n",
    "\n",
    "    # Convert to HSV color space and separate the V channel\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HLS).astype(np.float)\n",
    "    l_channel = hsv[:,:,1]\n",
    "    s_channel = hsv[:,:,2]\n",
    "\n",
    "    # Sobel x\n",
    "    sobelx = cv2.Sobel(l_channel, cv2.CV_64F, 1, 0) # Take the derivative in x\n",
    "    abs_sobelx = np.absolute(sobelx) # Absolute x derivative to accentuate lines away from horizontal\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "\n",
    "    # Threshold x gradient\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= sx_thresh[0]) & (scaled_sobel <= sx_thresh[1])] = 1\n",
    "\n",
    "    # Threshold color channel\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    # Stack each channel\n",
    "    # Note color_binary[:, :, 0] is all 0s, effectively an all black image. It might\n",
    "    # be beneficial to replace this channel with something else.\n",
    "    color_binary = np.dstack(( np.zeros_like(sxbinary), sxbinary, s_binary))\n",
    "\n",
    "    # Combine the two binary thresholds\n",
    "    combined_binary = np.zeros_like(sxbinary)\n",
    "    combined_binary[(s_binary == 1) | (sxbinary == 1)] =1\n",
    "    \n",
    "    return combined_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a6a76-1ffd-43de-9683-538219813429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with test\n",
    "thresh_test_image = mpimg.imread('test_images/test5.jpg')\n",
    "thresh_result_test = threshold(thresh_test_image)\n",
    "plt.imshow(thresh_result_test, cmap = plt.get_cmap('gray'))\n",
    "\n",
    "# Plot the result\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "\n",
    "ax1.imshow(thresh_test_image)\n",
    "ax1.set_title('Original Image', fontsize=40)\n",
    "\n",
    "ax2.imshow(thresh_result_test, cmap = plt.get_cmap('gray'))\n",
    "ax2.set_title('Combined S channel and gradient thresholds', fontsize=40)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "plt.savefig('output_images/threshold.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13a848-a335-4bbb-a183-3b2251b7229e",
   "metadata": {},
   "source": [
    "# Perspective Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54188106-e4ae-4957-9730-f3dd3b14295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in and display original image\n",
    "perspective_transform_img = mpimg.imread('test_images/straight_lines1.jpg')\n",
    "plt.imshow(perspective_transform_img)\n",
    "plt.show()\n",
    "\n",
    "# Source image points\n",
    "imshape = perspective_transform_img.shape\n",
    "plt.imshow(perspective_transform_img)\n",
    "plt.plot(681, 444, '.') # top right\n",
    "plt.plot(1200, imshape[0], '.') # bottom right\n",
    "plt.plot(598, 444, '.') # top left\n",
    "plt.plot(200, imshape[0], '.') # bottom left\n",
    "plt.savefig('output_images/srcPointsPlotted.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab386c39-81f6-4471-9ced-a27a7d30921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image for imshape\n",
    "img_for_src_size = mpimg.imread('test_images/test1.jpg')\n",
    "imshape = img_for_src_size.shape\n",
    "\n",
    "# Four source coordinates\n",
    "src = np.float32(\n",
    "    [[681, 444],\n",
    "     [1200, imshape[0]],\n",
    "     [598, 444],\n",
    "     [200, imshape[0]]])\n",
    "\n",
    "# Four desired coordinates\n",
    "dst = np.float32(\n",
    "    [[975, (imshape[0] - imshape[0])],\n",
    "     [975, imshape[0]],\n",
    "     [300, (imshape[0] - imshape[0])],\n",
    "     [300, imshape[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d08cf-4534-4b12-b905-0cd3dfeb4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define perspective transform function\n",
    "def warp(img):\n",
    "    # Define calibration box in source (original) and destination (desired or warped) coordinates\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    # Compute the perspective transform, M\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    # Compute the perspective transform inverse, Minv\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "\n",
    "    # Create warped image - uses linear interpolation\n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876a15a-319d-4478-a660-4e0727c29ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with test\n",
    "warped_im_test = warp(perspective_transform_img)\n",
    "plt.imshow(warped_im_test)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(20,10))\n",
    "\n",
    "ax1.set_title('Source image')\n",
    "ax1.imshow(perspective_transform_img)\n",
    "ax2.set_title('Warped image')\n",
    "ax2.imshow(warped_im_test)\n",
    "plt.savefig('output_images/perspectiveTransform.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be372168-8989-496d-9899-e5c96c9948ea",
   "metadata": {},
   "source": [
    "# Polygonal Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555acae-0269-4906-8902-dff5ee4cb811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image for image shape\n",
    "img = (mpimg.imread('test_images/straight_lines1.jpg'))\n",
    "imshape = img.shape\n",
    "\n",
    "# Four vertices for image masking\n",
    "vertices = np.array([[(100, imshape[0]),(560, 444),(720,444),(1200, imshape[0])]], dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67d9f6-097f-480b-8dd0-237d7df92359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_of_interest(img, vertices):\n",
    "    # Define a blank mask to start with\n",
    "    mask = np.zeros_like(img)\n",
    "    \n",
    "    # Define a 3 or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2] # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "\n",
    "    # Fill pixels inside the polygon defined by 'vertices' with the fill color\n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "\n",
    "    # return the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d78bd-0f34-4b7d-aa76-97f83a988789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with test\n",
    "\n",
    "maskTest = region_of_interest(img, vertices)\n",
    "plt.imshow(maskTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c260e-bbda-41e2-9608-7d7fe6ba5d67",
   "metadata": {},
   "source": [
    "# Measuring Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b401bed-54f1-4e44-9ed2-3ccce923b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some fake data to represent lane-line pixels\n",
    "ploty = np.linspace(0, 719, num=720)# to cover same y-range as image\n",
    "quadratic_coeff = 3e-4 # arbitrary quadratic coefficient\n",
    "# For each y position generate random x position within +/-50 pix\n",
    "# of the line base position in each case (x=200 for left, and x=900 for right)\n",
    "leftx = np.array([200 + (y**2)*quadratic_coeff + np.random.randint(-50, high=51) \n",
    "                              for y in ploty])\n",
    "rightx = np.array([900 + (y**2)*quadratic_coeff + np.random.randint(-50, high=51) \n",
    "                                for y in ploty])\n",
    "\n",
    "leftx = leftx[::-1]  # Reverse to match top-to-bottom in y\n",
    "rightx = rightx[::-1]  # Reverse to match top-to-bottom in y\n",
    "\n",
    "\n",
    "# Fit a second order polynomial to pixel positions in each fake lane line\n",
    "left_fit = np.polyfit(ploty, leftx, 2)\n",
    "left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "right_fit = np.polyfit(ploty, rightx, 2)\n",
    "right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "# Plot up the fake data\n",
    "mark_size = 3\n",
    "plt.plot(leftx, ploty, 'o', color='red', markersize=mark_size)\n",
    "plt.plot(rightx, ploty, 'o', color='blue', markersize=mark_size)\n",
    "plt.xlim(0, 1280)\n",
    "plt.ylim(0, 720)\n",
    "plt.plot(left_fitx, ploty, color='green', linewidth=3)\n",
    "plt.plot(right_fitx, ploty, color='green', linewidth=3)\n",
    "plt.gca().invert_yaxis() # to visualize as we do the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c5f3c-9b97-4443-ad84-2b234881045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y-value where we want radius of curvature\n",
    "# I'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "y_eval = np.max(ploty)\n",
    "left_curverad = ((1 + (2*left_fit[0]*y_eval + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0])\n",
    "right_curverad = ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "print(left_curverad, right_curverad)\n",
    "# Example values: 1926.74 1908.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0c155-d61c-49c5-afca-edfc57f96e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conversions in x and y from pixels space to meters\n",
    "ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "# Fit new polynomials to x,y in world space\n",
    "left_fit_cr = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "# Calculate the new radii of curvature\n",
    "left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "# Now our radius of curvature is in meters\n",
    "print(left_curverad, 'm', right_curverad, 'm')\n",
    "# Example values: 632.1 m    626.2 m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdacd99-6c63-4355-a0a9-60bd80de5f36",
   "metadata": {},
   "source": [
    "# Class to receive the characteristics of each line detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86165101-ac31-4bfb-9b2f-fc88acaafdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global left_lane \n",
    "global right_lane\n",
    "\n",
    "# Define a class to receive the characteristics of each line detection\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = np.zeros(720)\n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = np.zeros(3)  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = np.zeros(1)\n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = np.zeros(1)\n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "        #x values for detected line pixels\n",
    "        self.allx = None  \n",
    "        #y values for detected line pixels\n",
    "        self.ally = None\n",
    "        #smoothen the n frames \n",
    "        self.smoothen_nframes = 10\n",
    "        #first frame \n",
    "        self.first_frame = True\n",
    "        \n",
    "left_lane = Line()\n",
    "right_lane = Line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3379a8ee-5d48-4764-8cc2-ccdbb9c740ad",
   "metadata": {},
   "source": [
    "# Find Lanes Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a936e31-a3ac-4b69-9030-dc96ecb03a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lanes(binary_warped):\n",
    "    # Assuming you have created a warped binary image called \"binary_warped\"\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped[binary_warped.shape[0]//2:,:], axis=0)\n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set height of windows\n",
    "    window_height = np.int(binary_warped.shape[0]/nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        # Draw the windows on the visualization image\n",
    "        #cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        #cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "    #check for any lanes that are not detected in this current frame then use the history\n",
    "    if (leftx.size < 5):\n",
    "        left_lane.detected = False\n",
    "        #print (\"Left lane deteceted - False\")\n",
    "    else:\n",
    "        left_lane.detected = True\n",
    "        #print (\"Left lane detected - true\")\n",
    "    \n",
    "    if (rightx.size < 5):\n",
    "        right_lane.detected = False\n",
    "        #print (\"Right lane detected False\")\n",
    "    else:\n",
    "        right_lane.detected = True\n",
    "        #print (\"Right lane detected True\")\n",
    "        \n",
    "    #print (left_lane.detected, right_lane.detected)\n",
    "    #if lane is detected then try to fit the poly\n",
    "    if left_lane.detected == True & right_lane.detected == True:\n",
    "        # Fit a second order polynomial to each\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "        left_lane.best_fit = np.vstack([left_lane.best_fit,left_fit])\n",
    "        left_lane.best_fit[0] = left_fit\n",
    "        right_lane.best_fit = np.vstack([right_lane.best_fit,right_fit])\n",
    "        right_lane.best_fit[0] = right_fit\n",
    "        left_lane.best_fit = np.average(left_lane.best_fit[-left_lane.smoothen_nframes:], axis = 0)\n",
    "        right_lane.best_fit = np.average(right_lane.best_fit[-right_lane.smoothen_nframes:], axis = 0)\n",
    "        #print (\"saved best fit\")\n",
    "    else: \n",
    "        #use the history avg values \n",
    "        left_fit = left_lane.best_fit\n",
    "        right_fit = right_lane.best_fit\n",
    "        #print (\"used previous best fit\")\n",
    "    #calculate the actual points in x and y is from 0 to 719\n",
    "    ploty = np.linspace(0, out_img.shape[0]-1, out_img.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "    #print (left_lane.first_frame)\n",
    "    #Take the average here of the last n frames   \n",
    "    if left_lane.first_frame == True:\n",
    "        left_lane.first_frame = False\n",
    "        left_lane.bestx = np.vstack([left_lane.bestx,left_fitx])\n",
    "        left_lane.bestx[0] = left_fitx\n",
    "        #print (\"Inside first frame\")\n",
    "    \n",
    "    if ((left_fitx[0] > right_fitx[0]) | (abs(left_fitx[0] - right_fitx[0])<350) | (abs(left_fitx[0] - right_fitx[0])>700) | \n",
    "                   (left_fitx[0] > 800 )):\n",
    "        left_lane.bestx = np.vstack([left_lane.bestx,left_lane.bestx])  \n",
    "        #print (\"Skip lane left\")\n",
    "    else:\n",
    "        left_lane.bestx = np.vstack([left_lane.bestx,left_fitx])\n",
    "        #print (\"read lane left\")\n",
    "        \n",
    "    left_lane.bestx = np.average(left_lane.bestx[-left_lane.smoothen_nframes:], axis = 0)\n",
    "    \n",
    "    if right_lane.first_frame == True:\n",
    "        right_lane.first_frame = False\n",
    "        right_lane.bestx = np.vstack([right_lane.bestx,right_fitx])\n",
    "        right_lane.bestx[0] = right_fitx\n",
    "        #print (\"Inside first frame\")\n",
    "    \n",
    "    if ((left_fitx[0] > right_fitx[0]) | (abs(left_fitx[0] - right_fitx[0])<350) | (abs(left_fitx[0] - right_fitx[0])>700) |\n",
    "                      (right_fitx[0] > 1200)):\n",
    "        right_lane.bestx = np.vstack([right_lane.bestx,right_lane.bestx])\n",
    "        #print (\"Skip lane right\")\n",
    "    else:\n",
    "        right_lane.bestx = np.vstack([right_lane.bestx,right_fitx])\n",
    "        #print (\"read lane right\")\n",
    "    \n",
    "    right_lane.bestx = np.average(right_lane.bestx[-right_lane.smoothen_nframes:], axis = 0)\n",
    "    \n",
    "    #print (left_lane.bestx[0])\n",
    "    #print (left_fitx[0])\n",
    "    #print (right_lane.bestx[0])\n",
    "    #print (right_fitx[0])\n",
    "\n",
    "    \n",
    "    window_img = np.zeros_like(out_img)\n",
    "    margin = 10\n",
    "    \n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 255]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [255, 0, 255]\n",
    "    # Generate a polygon to illustrate the SMOOTHENED FIT\n",
    "    # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "    left_line_window1 = np.array([np.transpose(np.vstack([left_lane.bestx-margin, ploty]))])\n",
    "    left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_lane.bestx+margin, ploty])))])\n",
    "    left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "    right_line_window1 = np.array([np.transpose(np.vstack([right_lane.bestx-margin, ploty]))])\n",
    "    right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_lane.bestx+margin, ploty])))])\n",
    "    right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "    cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "    result = cv2.addWeighted(out_img, 1, window_img, 1, 0)\n",
    "    \n",
    "    # Generate a polygon to illustrate the CURRENT FRAME FIT\n",
    "    # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "    left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])\n",
    "    left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, ploty])))])\n",
    "    left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "    right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])\n",
    "    right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, ploty])))])\n",
    "    right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,0, 255))\n",
    "    cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,0, 255))\n",
    "    result = cv2.addWeighted(out_img, 1, window_img, 1, 0)\n",
    "    \n",
    "    #if the lane was detectded then calculate the curvatire or use the history\n",
    "    if (leftx.size > 2 | rightx.size > 2) :\n",
    "        y_eval = np.max(ploty)\n",
    "        # Define conversions in x and y from pixels space to meters\n",
    "        ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "        xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "        # Fit new polynomials to x,y in world space\n",
    "        left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "        right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "        # Calculate the new radii of curvature\n",
    "        left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "        right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "        # Now our radius of curvature is in meters and dist from centre\n",
    "        lane_centre = (left_fitx[-1] + right_fitx[-1])/2.0\n",
    "        camera_centre = result.shape[1]/2.0\n",
    "\n",
    "        dist_centre_val = (lane_centre - camera_centre)*3.7/700\n",
    "        avg_cur = (right_curverad+left_curverad)/2.0\n",
    "        \n",
    "        left_lane.line_base_pos = np.vstack([left_lane.line_base_pos,dist_centre_val])\n",
    "        left_lane.line_base_pos[0] = dist_centre_val\n",
    "        left_lane.line_base_pos = np.average(left_lane.line_base_pos[-left_lane.smoothen_nframes:], axis = 0)\n",
    "        \n",
    "        left_lane.radius_of_curvature = np.vstack([left_lane.radius_of_curvature,avg_cur])\n",
    "        left_lane.radius_of_curvature[0] = avg_cur\n",
    "        left_lane.radius_of_curvature = np.average(left_lane.radius_of_curvature[-left_lane.smoothen_nframes:], axis = 0)\n",
    "        \n",
    "    #print (avg_cur, dist_centre_val)\n",
    "    # else use the history curvature\n",
    "    else:\n",
    "        dist_centre_val = left_lane.line_base_pos\n",
    "        avg_cur = left_lane.radius_of_curvature\n",
    "    \n",
    "    #reset the lane detected to false for the next frame \n",
    "    left_lane.detected == False\n",
    "    right_lane.detected == False\n",
    "    return result, left_lane.bestx, right_lane.bestx, ploty, left_lane.radius_of_curvature, left_lane.line_base_pos\n",
    "\n",
    "\n",
    "# Draw Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf043e-bae3-407e-bd60-9aabaab0f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_on_original(undist, left_fitx, right_fitx, ploty,Minv):\n",
    "    # Create an image to draw the lines on\n",
    "    color_warp = np.zeros_like(undist).astype(np.uint8)\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane with low confidence region in Google Red ;)\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (219, 50, 54))\n",
    "    \n",
    "    #confidence region in Google Blue :) \n",
    "    shift = 50\n",
    "    diff = (right_fitx - left_fitx)/2\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx[400:], ploty[400:]]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx[400:], ploty[400:]])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (72, 133, 237))\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (undist.shape[1], undist.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(undist, 1, newwarp, 0.4, 0)\n",
    "    plt.imshow(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6b5d38-7119-4be9-86f2-7c98a732544a",
   "metadata": {},
   "source": [
    "# Software Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf7841-6918-4e27-b229-95aa52e6a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(img):\n",
    "    undist, _, _ = cal_undistort(img, objpoints, imgpoints)\n",
    "    threshed = threshold(undist)\n",
    "    masked = region_of_interest(threshed,vertices)\n",
    "    perspective = warp(masked)\n",
    "\n",
    "    # Explicitly defining Minv\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    # perspective = cv2.cvtColor(perspective, cv2.COLOR_RGB2GRAY).astype(np.uint8)\n",
    "    #pass the perspective image to the lane fitting stage\n",
    "    slides_pers, left_fitx, right_fitx, ploty, avg_cur, dist_centre_val = find_lanes(perspective)\n",
    "    #draw the detected lanes on the original image \n",
    "    mapped_lane = draw_on_original(undist, left_fitx, right_fitx, ploty, Minv)\n",
    "    #font and text for drawing the offset and curvature \n",
    "    curvature = \"Estimated lane curvature %.2fm\" % (avg_cur)\n",
    "    dist_centre = \"Estimated offset from lane center %.2fm\" % (dist_centre_val)\n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    # using cv2 for drawing text/images in diagnostic pipeline.\n",
    "    #else return the original mapped imaged with the curvature and offset drawn, in Google Colors!\n",
    "    cv2.putText(mapped_lane, curvature, (30, 60), font, 1.2, (60,186,84), 2)\n",
    "    cv2.putText(mapped_lane, dist_centre, (30, 120), font, 1.2, (244,194,13), 2)\n",
    "    \n",
    "    return mapped_lane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df247a9f-bbbe-4850-9977-000323f830be",
   "metadata": {},
   "source": [
    "# Test Pipeline on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3df36-0e07-4a3b-9df6-a28a98b152d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_final = mpimg.imread('test_images/straight_lines1.jpg')\n",
    "test = pipeline(test_image_final)\n",
    "# plt.imshow(test)\n",
    "# plt.savefig('output_images/lanesDrawnCurvatureEstimatedDistanceFromCenter.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d561b-25c4-42a9-a889-223ceda0bb10",
   "metadata": {},
   "source": [
    "# Test Pipeline on Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba761d-cc69-43d9-b002-97519bfd9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4dd186-89c2-44aa-9ca3-aee5b6532b06",
   "metadata": {},
   "source": [
    "output_vid = 'output_images/project_submission_video.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "output_clip = clip1.fl_image(pipeline)\n",
    "%time output_clip.write_videofile(output_vid, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f490605e-1b8c-47ac-934b-75639cc6e2ed",
   "metadata": {},
   "source": [
    "# Display Video in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a46f9c-0d0c-42b7-9d26-143799ab7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output_vid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305a6e3-585c-4249-9301-c316dda9a538",
   "metadata": {},
   "source": [
    "# Tests to save images to output_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdff933-8122-4734-bbaa-4e64a57c9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first Image\n",
    "initial_img = mpimg.imread('test_images/test2.jpg')\n",
    "plt.imshow(initial_img)\n",
    "plt.savefig('output_images/1nitial_img.jpg')\n",
    "\n",
    "# undistort\n",
    "undistorted_test, _, _ = cal_undistort(initial_img, objpoints, imgpoints)\n",
    "plt.imshow(undistorted_test)\n",
    "plt.savefig('output_images/2undistort.jpg')\n",
    "\n",
    "# threshold\n",
    "thresh_test = threshold(undistorted_test)\n",
    "plt.imshow(thresh_test, cmap = plt.get_cmap('gray'))\n",
    "plt.savefig('output_images/3threhsold.jpg')\n",
    "\n",
    "# mask\n",
    "masked_test = region_of_interest(thresh_test, vertices)\n",
    "plt.imshow(masked_test, cmap = plt.get_cmap('gray'))\n",
    "plt.savefig('output_images/4mask.jpg')\n",
    "\n",
    "# perspective warp\n",
    "perspective_test = warp(masked_test)\n",
    "plt.imshow(perspective_test, cmap = plt.get_cmap('gray'))\n",
    "plt.savefig('output_images/5perspectivewarp.jpg')\n",
    "\n",
    "\n",
    "# Find lanes\n",
    "find_lanes_from_tests = find_lanes(perspective_test)\n",
    "# saved it in function #6\n",
    "\n",
    "\n",
    "# draw lanes and info\n",
    "finished_img = pipeline(initial_img)\n",
    "plt.imshow(finished_img)\n",
    "plt.savefig('output_images/7finalImage.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
